{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pCQxvGgLh-qP"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyjT92jfh-qR"
      },
      "source": [
        "# **Profiling your PyTorch Module**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_F2bZd34h-qT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.autograd.profiler as profiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM0Qc6aah-qT"
      },
      "source": [
        "Performance debugging using Profiler\n",
        "====================================\n",
        "\n",
        "Profiler can be useful to identify performance bottlenecks in your\n",
        "models. In this example, we build a custom module that performs two\n",
        "sub-tasks:\n",
        "\n",
        "-   a linear transformation on the input, and\n",
        "-   use the transformation result to get indices on a mask tensor.\n",
        "\n",
        "We wrap the code for each sub-task in separate labelled context managers\n",
        "using `profiler.record_function(\"label\")`. In the profiler output, the\n",
        "aggregate performance metrics of all operations in the sub-task will\n",
        "show up under its corresponding label.\n",
        "\n",
        "Note that using Profiler incurs some overhead, and is best used only for\n",
        "investigating code. Remember to remove it if you are benchmarking\n",
        "runtimes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3R0ADb9Mh-qU"
      },
      "outputs": [],
      "source": [
        "class MyModule(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        with profiler.record_function(\"LINEAR PASS\"):\n",
        "            out = self.linear(input)\n",
        "\n",
        "        with profiler.record_function(\"MASK INDICES\"):\n",
        "            threshold = out.sum(axis=1).mean().item()\n",
        "            hi_idx = np.argwhere(mask.cpu().numpy() > threshold)\n",
        "            hi_idx = torch.from_numpy(hi_idx).cuda()\n",
        "\n",
        "        return out, hi_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAPz4Azwh-qU"
      },
      "source": [
        "Profile the forward pass\n",
        "========================\n",
        "\n",
        "We initialize random input and mask tensors, and the model.\n",
        "\n",
        "Before we run the profiler, we warm-up CUDA to ensure accurate\n",
        "performance benchmarking. We wrap the forward pass of our module in the\n",
        "`profiler.profile` context manager. The `with_stack=True` parameter\n",
        "appends the file and line number of the operation in the trace.\n",
        "\n",
        "<div style=\"background-color: #e94f3b; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>WARNING:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p><code>with_stack=True</code> incurs an additional overhead, and is better suited for investigating code.Remember to remove it if you are benchmarking performance.</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tMIvXK-rh-qU"
      },
      "outputs": [],
      "source": [
        "model = MyModule(500, 10).cuda()\n",
        "input = torch.rand(128, 500).cuda()\n",
        "mask = torch.rand((500, 500, 500), dtype=torch.double).cuda()\n",
        "\n",
        "# warm-up\n",
        "model(input, mask)\n",
        "\n",
        "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "    out, idx = model(input, mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVmxgyewh-qU"
      },
      "source": [
        "Print profiler results\n",
        "======================\n",
        "\n",
        "Finally, we print the profiler results. `profiler.key_averages`\n",
        "aggregates the results by operator name, and optionally by input shapes\n",
        "and/or stack trace events. Grouping by input shapes is useful to\n",
        "identify which tensor shapes are utilized by the model.\n",
        "\n",
        "Here, we use `group_by_stack_n=5` which aggregates runtimes by the\n",
        "operation and its traceback (truncated to the most recent 5 events), and\n",
        "display the events in the order they are registered. The table can also\n",
        "be sorted by passing a `sort_by` argument (refer to the\n",
        "[docs](https://pytorch.org/docs/stable/autograd.html#profiler) for valid\n",
        "sorting keys).\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "<p>When running profiler in a notebook, you might see entries like <code>&lt;ipython-input-18-193a910735e8&gt;(13): forward</code>instead of filenames in the stacktrace. These correspond to <code>&lt;notebook-cell&gt;(line number): calling-function</code>.</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "JcZaXDknh-qU",
        "outputId": "60d43dec-a5bc-4f99-9127-fd1aa8ffc7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                     MASK INDICES        67.10%        2.730s        99.83%        4.061s        4.061s           0 b    -953.67 Mb       2.79 Gb      -1.00 Kb             1  \n",
            "                                  cudaMemcpyAsync        32.71%        1.331s        32.71%        1.331s     443.548ms           0 b           0 b           0 b           0 b             3  \n",
            "                                      aten::addmm         0.06%       2.387ms         0.07%       3.048ms       3.048ms           0 b           0 b       5.00 Kb       5.00 Kb             1  \n",
            "                                     aten::linear         0.05%       2.028ms         0.13%       5.125ms       5.125ms           0 b           0 b       5.00 Kb           0 b             1  \n",
            "                                      LINEAR PASS         0.05%       1.834ms         0.17%       6.959ms       6.959ms           0 b           0 b       5.00 Kb           0 b             1  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 4.068s\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(Some columns are omitted)\\n\\n-------------  ------------  ------------  ------------  ---------------------------------\\n         Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\\n-------------  ------------  ------------  ------------  ---------------------------------\\n MASK INDICES        87.88%        5.212s    -953.67 Mb  /mnt/xarfuse/.../torch/au\\n                                                         <ipython-input-...>(10): forward\\n                                                         /mnt/xarfuse/.../torch/nn\\n                                                         <ipython-input-...>(9): <module>\\n                                                         /mnt/xarfuse/.../IPython/\\n\\n  aten::copy_        12.07%     715.848ms           0 b  <ipython-input-...>(12): forward\\n                                                         /mnt/xarfuse/.../torch/nn\\n                                                         <ipython-input-...>(9): <module>\\n                                                         /mnt/xarfuse/.../IPython/\\n                                                         /mnt/xarfuse/.../IPython/\\n\\n  LINEAR PASS         0.01%     350.151us         -20 b  /mnt/xarfuse/.../torch/au\\n                                                         <ipython-input-...>(7): forward\\n                                                         /mnt/xarfuse/.../torch/nn\\n                                                         <ipython-input-...>(9): <module>\\n                                                         /mnt/xarfuse/.../IPython/\\n\\n  aten::addmm         0.00%     293.342us           0 b  /mnt/xarfuse/.../torch/nn\\n                                                         /mnt/xarfuse/.../torch/nn\\n                                                         /mnt/xarfuse/.../torch/nn\\n                                                         <ipython-input-...>(8): forward\\n                                                         /mnt/xarfuse/.../torch/nn\\n\\n   aten::mean         0.00%     235.095us           0 b  <ipython-input-...>(11): forward\\n                                                         /mnt/xarfuse/.../torch/nn\\n                                                         <ipython-input-...>(9): <module>\\n                                                         /mnt/xarfuse/.../IPython/\\n                                                         /mnt/xarfuse/.../IPython/\\n\\n-----------------------------  ------------  ---------- ----------------------------------\\nSelf CPU time total: 5.931s\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n",
        "\n",
        "\"\"\"\n",
        "(Some columns are omitted)\n",
        "\n",
        "-------------  ------------  ------------  ------------  ---------------------------------\n",
        "         Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n",
        "-------------  ------------  ------------  ------------  ---------------------------------\n",
        " MASK INDICES        87.88%        5.212s    -953.67 Mb  /mnt/xarfuse/.../torch/au\n",
        "                                                         <ipython-input-...>(10): forward\n",
        "                                                         /mnt/xarfuse/.../torch/nn\n",
        "                                                         <ipython-input-...>(9): <module>\n",
        "                                                         /mnt/xarfuse/.../IPython/\n",
        "\n",
        "  aten::copy_        12.07%     715.848ms           0 b  <ipython-input-...>(12): forward\n",
        "                                                         /mnt/xarfuse/.../torch/nn\n",
        "                                                         <ipython-input-...>(9): <module>\n",
        "                                                         /mnt/xarfuse/.../IPython/\n",
        "                                                         /mnt/xarfuse/.../IPython/\n",
        "\n",
        "  LINEAR PASS         0.01%     350.151us         -20 b  /mnt/xarfuse/.../torch/au\n",
        "                                                         <ipython-input-...>(7): forward\n",
        "                                                         /mnt/xarfuse/.../torch/nn\n",
        "                                                         <ipython-input-...>(9): <module>\n",
        "                                                         /mnt/xarfuse/.../IPython/\n",
        "\n",
        "  aten::addmm         0.00%     293.342us           0 b  /mnt/xarfuse/.../torch/nn\n",
        "                                                         /mnt/xarfuse/.../torch/nn\n",
        "                                                         /mnt/xarfuse/.../torch/nn\n",
        "                                                         <ipython-input-...>(8): forward\n",
        "                                                         /mnt/xarfuse/.../torch/nn\n",
        "\n",
        "   aten::mean         0.00%     235.095us           0 b  <ipython-input-...>(11): forward\n",
        "                                                         /mnt/xarfuse/.../torch/nn\n",
        "                                                         <ipython-input-...>(9): <module>\n",
        "                                                         /mnt/xarfuse/.../IPython/\n",
        "                                                         /mnt/xarfuse/.../IPython/\n",
        "\n",
        "-----------------------------  ------------  ---------- ----------------------------------\n",
        "Self CPU time total: 5.931s\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFV-fih-h-qV"
      },
      "source": [
        "Improve memory performance\n",
        "==========================\n",
        "\n",
        "Note that the most expensive operations - in terms of memory and time\n",
        "-are at `forward (10)` representing the operations within MASK INDICES.\n",
        "Let's try to tackle the memory consumption first. We can see that the\n",
        "`.to()` operation at line 12 consumes 953.67 Mb. This operation copies\n",
        "`mask` to the CPU. `mask` is initialized with a `torch.double` datatype.\n",
        "Can we reduce the memory footprint by casting it to `torch.float`\n",
        "instead?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "dJ-vTehSh-qV",
        "outputId": "1912032d-040e-46d0-dd1f-4542e2dcde56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                     MASK INDICES        67.22%     896.462ms        99.76%        1.330s        1.330s           0 b    -476.84 Mb     337.94 Mb      -1.00 Kb             1  \n",
            "                                  cudaMemcpyAsync        32.50%     433.477ms        32.50%     433.477ms     144.492ms           0 b           0 b           0 b           0 b             3  \n",
            "                                      aten::addmm         0.22%       2.949ms         0.22%       2.985ms       2.985ms           0 b           0 b       5.00 Kb       5.00 Kb             1  \n",
            "                                      LINEAR PASS         0.01%     158.000us         0.24%       3.203ms       3.203ms           0 b           0 b       5.00 Kb           0 b             1  \n",
            "                                      aten::copy_         0.01%     102.000us        32.52%     433.648ms     216.824ms           0 b           0 b           0 b           0 b             2  \n",
            "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.334s\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(Some columns are omitted)\\n\\n-----------------  ------------  ------------  ------------  --------------------------------\\n             Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\\n-----------------  ------------  ------------  ------------  --------------------------------\\n     MASK INDICES        93.61%        5.006s    -476.84 Mb  /mnt/xarfuse/.../torch/au\\n                                                             <ipython-input-...>(10): forward\\n                                                             /mnt/xarfuse/  /torch/nn\\n                                                             <ipython-input-...>(9): <module>\\n                                                             /mnt/xarfuse/.../IPython/\\n\\n      aten::copy_         6.34%     338.759ms           0 b  <ipython-input-...>(12): forward\\n                                                             /mnt/xarfuse/.../torch/nn\\n                                                             <ipython-input-...>(9): <module>\\n                                                             /mnt/xarfuse/.../IPython/\\n                                                             /mnt/xarfuse/.../IPython/\\n\\n aten::as_strided         0.01%     281.808us           0 b  <ipython-input-...>(11): forward\\n                                                             /mnt/xarfuse/.../torch/nn\\n                                                             <ipython-input-...>(9): <module>\\n                                                             /mnt/xarfuse/.../IPython/\\n                                                             /mnt/xarfuse/.../IPython/\\n\\n      aten::addmm         0.01%     275.721us           0 b  /mnt/xarfuse/.../torch/nn\\n                                                             /mnt/xarfuse/.../torch/nn\\n                                                             /mnt/xarfuse/.../torch/nn\\n                                                             <ipython-input-...>(8): forward\\n                                                             /mnt/xarfuse/.../torch/nn\\n\\n      aten::_local        0.01%     268.650us           0 b  <ipython-input-...>(11): forward\\n      _scalar_dense                                          /mnt/xarfuse/.../torch/nn\\n                                                             <ipython-input-...>(9): <module>\\n                                                             /mnt/xarfuse/.../IPython/\\n                                                             /mnt/xarfuse/.../IPython/\\n\\n-----------------  ------------  ------------  ------------  --------------------------------\\nSelf CPU time total: 5.347s\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model = MyModule(500, 10).cuda()\n",
        "input = torch.rand(128, 500).cuda()\n",
        "mask = torch.rand((500, 500, 500), dtype=torch.float).cuda()\n",
        "\n",
        "# warm-up\n",
        "model(input, mask)\n",
        "\n",
        "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "    out, idx = model(input, mask)\n",
        "\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n",
        "\n",
        "\"\"\"\n",
        "(Some columns are omitted)\n",
        "\n",
        "-----------------  ------------  ------------  ------------  --------------------------------\n",
        "             Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n",
        "-----------------  ------------  ------------  ------------  --------------------------------\n",
        "     MASK INDICES        93.61%        5.006s    -476.84 Mb  /mnt/xarfuse/.../torch/au\n",
        "                                                             <ipython-input-...>(10): forward\n",
        "                                                             /mnt/xarfuse/  /torch/nn\n",
        "                                                             <ipython-input-...>(9): <module>\n",
        "                                                             /mnt/xarfuse/.../IPython/\n",
        "\n",
        "      aten::copy_         6.34%     338.759ms           0 b  <ipython-input-...>(12): forward\n",
        "                                                             /mnt/xarfuse/.../torch/nn\n",
        "                                                             <ipython-input-...>(9): <module>\n",
        "                                                             /mnt/xarfuse/.../IPython/\n",
        "                                                             /mnt/xarfuse/.../IPython/\n",
        "\n",
        " aten::as_strided         0.01%     281.808us           0 b  <ipython-input-...>(11): forward\n",
        "                                                             /mnt/xarfuse/.../torch/nn\n",
        "                                                             <ipython-input-...>(9): <module>\n",
        "                                                             /mnt/xarfuse/.../IPython/\n",
        "                                                             /mnt/xarfuse/.../IPython/\n",
        "\n",
        "      aten::addmm         0.01%     275.721us           0 b  /mnt/xarfuse/.../torch/nn\n",
        "                                                             /mnt/xarfuse/.../torch/nn\n",
        "                                                             /mnt/xarfuse/.../torch/nn\n",
        "                                                             <ipython-input-...>(8): forward\n",
        "                                                             /mnt/xarfuse/.../torch/nn\n",
        "\n",
        "      aten::_local        0.01%     268.650us           0 b  <ipython-input-...>(11): forward\n",
        "      _scalar_dense                                          /mnt/xarfuse/.../torch/nn\n",
        "                                                             <ipython-input-...>(9): <module>\n",
        "                                                             /mnt/xarfuse/.../IPython/\n",
        "                                                             /mnt/xarfuse/.../IPython/\n",
        "\n",
        "-----------------  ------------  ------------  ------------  --------------------------------\n",
        "Self CPU time total: 5.347s\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FhmC0Tth-qW"
      },
      "source": [
        "The CPU memory footprint for this operation has halved.\n",
        "\n",
        "Improve time performance\n",
        "========================\n",
        "\n",
        "While the time consumed has also reduced a bit, it's still too high.\n",
        "Turns out copying a matrix from CUDA to CPU is pretty expensive! The\n",
        "`aten::copy_` operator in `forward (12)` copies `mask` to CPU so that it\n",
        "can use the NumPy `argwhere` function. `aten::copy_` at `forward(13)`\n",
        "copies the array back to CUDA as a tensor. We could eliminate both of\n",
        "these if we use a `torch` function `nonzero()` here instead.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "RjLlFKl2h-qW",
        "outputId": "35835121-6835-4c80-e31b-6a92061f0f88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                        cudaMemcpyAsync        75.37%      25.585ms        75.37%      25.585ms      25.585ms           0 b           0 b           0 b           0 b             1  \n",
            "                                           aten::unbind         8.28%       2.809ms         8.32%       2.823ms       2.823ms           0 b           0 b           0 b           0 b             1  \n",
            "                                            aten::addmm         7.84%       2.662ms         7.92%       2.687ms       2.687ms           0 b           0 b       5.00 Kb       5.00 Kb             1  \n",
            "                                          aten::nonzero         6.42%       2.178ms        82.35%      27.954ms      27.954ms           0 b           0 b       2.79 Gb           0 b             1  \n",
            "                                            LINEAR PASS         0.52%     175.000us         8.58%       2.911ms       2.911ms           0 b           0 b       5.00 Kb           0 b             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 33.945ms\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n(Some columns are omitted)\\n\\n--------------  ------------  ------------  ------------  ---------------------------------\\n          Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\\n--------------  ------------  ------------  ------------  ---------------------------------\\n      aten::gt        57.17%     129.089ms           0 b  <ipython-input-...>(12): forward\\n                                                          /mnt/xarfuse/.../torch/nn\\n                                                          <ipython-input-...>(25): <module>\\n                                                          /mnt/xarfuse/.../IPython/\\n                                                          /mnt/xarfuse/.../IPython/\\n\\n aten::nonzero        37.38%      84.402ms           0 b  <ipython-input-...>(12): forward\\n                                                          /mnt/xarfuse/.../torch/nn\\n                                                          <ipython-input-...>(25): <module>\\n                                                          /mnt/xarfuse/.../IPython/\\n                                                          /mnt/xarfuse/.../IPython/\\n\\n   INDEX SCORE         3.32%       7.491ms    -119.21 Mb  /mnt/xarfuse/.../torch/au\\n                                                          <ipython-input-...>(10): forward\\n                                                          /mnt/xarfuse/.../torch/nn\\n                                                          <ipython-input-...>(25): <module>\\n                                                          /mnt/xarfuse/.../IPython/\\n\\naten::as_strided         0.20%    441.587us          0 b  <ipython-input-...>(12): forward\\n                                                          /mnt/xarfuse/.../torch/nn\\n                                                          <ipython-input-...>(25): <module>\\n                                                          /mnt/xarfuse/.../IPython/\\n                                                          /mnt/xarfuse/.../IPython/\\n\\n aten::nonzero\\n     _numpy             0.18%     395.602us           0 b  <ipython-input-...>(12): forward\\n                                                          /mnt/xarfuse/.../torch/nn\\n                                                          <ipython-input-...>(25): <module>\\n                                                          /mnt/xarfuse/.../IPython/\\n                                                          /mnt/xarfuse/.../IPython/\\n--------------  ------------  ------------  ------------  ---------------------------------\\nSelf CPU time total: 225.801ms\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "class MyModule(nn.Module):\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
        "        super(MyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features, bias)\n",
        "\n",
        "    def forward(self, input, mask):\n",
        "        with profiler.record_function(\"LINEAR PASS\"):\n",
        "            out = self.linear(input)\n",
        "\n",
        "        with profiler.record_function(\"MASK INDICES\"):\n",
        "            threshold = out.sum(axis=1).mean()\n",
        "            hi_idx = (mask > threshold).nonzero(as_tuple=True)\n",
        "\n",
        "        return out, hi_idx\n",
        "\n",
        "\n",
        "model = MyModule(500, 10).cuda()\n",
        "input = torch.rand(128, 500).cuda()\n",
        "mask = torch.rand((500, 500, 500), dtype=torch.float).cuda()\n",
        "\n",
        "# warm-up\n",
        "model(input, mask)\n",
        "\n",
        "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
        "    out, idx = model(input, mask)\n",
        "\n",
        "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n",
        "\n",
        "\"\"\"\n",
        "(Some columns are omitted)\n",
        "\n",
        "--------------  ------------  ------------  ------------  ---------------------------------\n",
        "          Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n",
        "--------------  ------------  ------------  ------------  ---------------------------------\n",
        "      aten::gt        57.17%     129.089ms           0 b  <ipython-input-...>(12): forward\n",
        "                                                          /mnt/xarfuse/.../torch/nn\n",
        "                                                          <ipython-input-...>(25): <module>\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "\n",
        " aten::nonzero        37.38%      84.402ms           0 b  <ipython-input-...>(12): forward\n",
        "                                                          /mnt/xarfuse/.../torch/nn\n",
        "                                                          <ipython-input-...>(25): <module>\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "\n",
        "   INDEX SCORE         3.32%       7.491ms    -119.21 Mb  /mnt/xarfuse/.../torch/au\n",
        "                                                          <ipython-input-...>(10): forward\n",
        "                                                          /mnt/xarfuse/.../torch/nn\n",
        "                                                          <ipython-input-...>(25): <module>\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "\n",
        "aten::as_strided         0.20%    441.587us          0 b  <ipython-input-...>(12): forward\n",
        "                                                          /mnt/xarfuse/.../torch/nn\n",
        "                                                          <ipython-input-...>(25): <module>\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "\n",
        " aten::nonzero\n",
        "     _numpy             0.18%     395.602us           0 b  <ipython-input-...>(12): forward\n",
        "                                                          /mnt/xarfuse/.../torch/nn\n",
        "                                                          <ipython-input-...>(25): <module>\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "                                                          /mnt/xarfuse/.../IPython/\n",
        "--------------  ------------  ------------  ------------  ---------------------------------\n",
        "Self CPU time total: 225.801ms\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkpR59N5h-qW"
      },
      "source": [
        "Further Reading\n",
        "===============\n",
        "\n",
        "We have seen how Profiler can be used to investigate time and memory\n",
        "bottlenecks in PyTorch models. Read more about Profiler here:\n",
        "\n",
        "-   [Profiler Usage\n",
        "    Recipe](https://pytorch.org/tutorials/recipes/recipes/profiler.html)\n",
        "-   [Profiling RPC-Based\n",
        "    Workloads](https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html)\n",
        "-   [Profiler API\n",
        "    Docs](https://pytorch.org/docs/stable/autograd.html?highlight=profiler#profiler)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}